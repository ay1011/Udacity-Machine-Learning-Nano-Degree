{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7b0e7606-d591-ad0a-3340-7a98a6f0b519",
    "_uuid": "092ca5b6a2c682b02dbd77fe8223f167b12e8ac5",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import missingno as msno\n",
    "from datetime import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kendalltau\n",
    "import warnings\n",
    "#matplotlib.style.use('ggplot')\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "color = sns.color_palette(\"hls\", 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "# import cufflinks and offline mode\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "\n",
    "\n",
    "print('plotly version:', __version__) \n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9ed1b204-7eb1-ee23-073a-8085a51d0804",
    "_uuid": "fa7575bd92e62040a16d5c4db41885d386ccc34b"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ed52ad2-5d37-1c05-edca-5d200f497e7b",
    "_uuid": "ad86c1d756864be51206a359d0f56083376f3a71",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "prop_df = pd.read_csv(\"properties_2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (\"Shape Of Train: \", train_df.shape)\n",
    "print (\"Shape Of Properties: \", prop_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "553701b1-1508-fb32-7212-4d6c4f6ab796",
    "_uuid": "b2bb699589d59de1641d35c6eebe62160e4c11aa",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of real estate properties in 3 counties (Los Angeles, Orange and Ventura, California) data in 2016.\n",
    "\n",
    "90,275 rows in train, 2,985,217 rows in properties file. Merge 2 files and then carry out analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df,prop_df,on=\"parcelid\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 65\n",
    "\n",
    "dtype_df = train_df.dtypes.reset_index()\n",
    "dtype_df.columns = [\"Count\", \"Column Type\"]\n",
    "dtype_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtype_df.groupby(\"Column Type\").aggregate('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataTypeDf = pd.DataFrame(train_df.dtypes.value_counts()).reset_index().rename(columns={\"index\":\"variableType\",0:\"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a trace\n",
    "trace = go.Bar(\n",
    "    x = dataTypeDf[\"variableType\"].astype(str),\n",
    "    y = dataTypeDf[\"count\"],\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = \"Variables Count Across Datatype\",\n",
    "              xaxis = dict(title = \"VariableType\"),\n",
    "              yaxis = dict(title = \"Count\"),\n",
    "              font = dict(size=15),\n",
    "              autosize = False,\n",
    "              width = 800,\n",
    "              height = 500,\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_df         = train_df.isnull().sum(axis=0).reset_index()\n",
    "missing_df.columns = ['column_name', 'missing_count']\n",
    "\n",
    "missing_df['missing_ratio'] = missing_df['missing_count'] / train_df.shape[0]\n",
    "missing_df.loc[missing_df['missing_ratio']>0.995]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 columns have missing values 99.9% of the times.!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7a04bd96-470e-bfc3-8099-a0b6e774ea6a",
    "_uuid": "dea28e79705c0ce172adc7a3a5c560e5d2e2d9b1"
   },
   "source": [
    "**Logerror:**\n",
    "\n",
    "Target variable for this competition is \"logerror\" field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a trace\n",
    "trace = go.Scatter(\n",
    "    x = range(train_df.shape[0]),\n",
    "    y = np.sort(train_df.logerror.values),\n",
    "    #mode= 'markers',\n",
    "    #marker= dict(size= 4,                 line= dict(width=1),                 opacity= 0.3,                )\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Logerror distribution',\n",
    "              xaxis = dict(title = 'index'),\n",
    "              yaxis = dict(title = 'logerror'),\n",
    "              font = dict(size=16),\n",
    "              autosize = False,\n",
    "              width = 600,\n",
    "              height = 500,\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1cce93eb-0687-fcc0-983e-60a2ea59b87e",
    "_uuid": "22626e6dc27c58d6d786b359a0410909bf5513e6"
   },
   "source": [
    "Outliers at both the ends!\n",
    "\n",
    "Remove the outliers and then do a histogram plot on the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "67ad699f-2314-883d-6cf3-b15f2ebdc7d7",
    "_uuid": "baf9546877caf208a2ffa9a3f336b19a6ccaeed4",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ulimit = np.percentile(train_df.logerror.values, 99)\n",
    "llimit = np.percentile(train_df.logerror.values, 1)\n",
    "train_df['logerror'].ix[train_df['logerror']>ulimit] = ulimit\n",
    "train_df['logerror'].ix[train_df['logerror']<llimit] = llimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "hist_data = [train_df.logerror.values]\n",
    "\n",
    "group_labels = ['logerror']\n",
    "\n",
    "# Create distplot with custom bin_size\n",
    "fig = ff.create_distplot(hist_data, group_labels, bin_size=.01)\n",
    "\n",
    "# Plot!\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [go.Histogram(x=train_df.logerror.values)]\n",
    "iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ca4e0aa-8652-cb13-5ca8-88ef50794147",
    "_uuid": "c2a7ad2d5706a0ff9523d0a627991a019f2d0679"
   },
   "source": [
    "**Transaction Date:**\n",
    "\n",
    "Date field. Check number of transactions in each month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2b641d53-1080-8141-47a9-065eb66ebd09",
    "_uuid": "35f493d5082a4fd720168fffb5c43bbbe72475ae",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df['transaction_month'] = train_df['transactiondate'].dt.month\n",
    "\n",
    "cnt_srs = train_df['transaction_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Bar(\n",
    "        x=cnt_srs.index, \n",
    "        y=cnt_srs.values,\n",
    "    )]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Transaction distribution',\n",
    "              xaxis = dict(title = 'Month of transaction'),\n",
    "              yaxis = dict(title = 'Number of Occurrences'),\n",
    "              font  = dict(size=16),\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7d525f89-1091-d0a7-1dc2-ade5c5027259",
    "_uuid": "46be43bba81a6d71f2d9f928cf9ed9271e42543c"
   },
   "source": [
    "Train data has all transactions before October 15, 2016, and some of the transactions after October 15, 2016.\n",
    "\n",
    "So shorter bars in last 3 months. \n",
    "\n",
    "**Parcel Id:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e3055a80-3151-0a0e-c388-9c77105cea8c",
    "_uuid": "03fdd009be5f2a1ccf504d6020ab3d6a2405d5b9",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(train_df['parcelid'].value_counts().reset_index())['parcelid'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b250fd92-b9ac-da88-5faa-69c22ef142b4",
    "_uuid": "1ff18604cbd91461735b2d9b4090cff7965cb14f"
   },
   "source": [
    "Most parcel ids are appearing only once in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5a4e7b91-5ee2-f972-af70-08e660e93162",
    "_uuid": "06c3cc36176e7aca50b202e014235647b62b673f"
   },
   "source": [
    "### Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missingValueColumns = train_df.columns[train_df.isnull().any()]\n",
    "msno.bar(train_df[missingValueColumns],\\\n",
    "            figsize=(20,8),color='blue',fontsize=12,labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f13cac6-5d01-fc2d-73be-98472e5a988a",
    "_uuid": "1077f2f877214441cb466a6d9d56592afe1c8912",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "missing_df         = prop_df.isnull().sum(axis=0).reset_index()\n",
    "missing_df.columns = ['column_name', 'missing_count']\n",
    "missing_df         = missing_df.loc[missing_df['missing_count']>0]\n",
    "missing_df         = missing_df.sort_values(by='missing_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [go.Bar(\n",
    "            x = missing_df.missing_count.values,\n",
    "            y = missing_df.column_name,\n",
    "            orientation = 'h',\n",
    "            )]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = \"Number of missing values in each column\",\n",
    "              xaxis = dict(title = \"Count of missing values\"),\n",
    "              yaxis = dict(tickangle=35, \n",
    "                           tickfont=dict(size=9)),\n",
    "              font  = dict(size=8),\n",
    "              autosize = False,\n",
    "              width = 900,\n",
    "              height = 990,\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2abd4104-622c-9ada-b397-b756664f69d3",
    "_uuid": "a23e20bf95d05d01abc4cf92b1f664ab447f1f8d"
   },
   "source": [
    "**Univariate Analysis:**\n",
    "\n",
    "Since there are so many variables, investigate 'float' variables alone and then get the correlation with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a6592dca-8f7a-cd6b-23df-d87a98ebf934",
    "_uuid": "c0e606965fa77c84e0204976a071d5c1559ab4f4",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let us just impute the missing values with mean values to compute correlation coefficients #\n",
    "mean_values  = train_df.mean(axis=0)\n",
    "train_df_new = train_df.fillna(mean_values, inplace=True)\n",
    "\n",
    "# Now let us look at the correlation coefficient of each of these variables #\n",
    "x_cols = [col for col in train_df_new.columns if col not in ['logerror'] if train_df_new[col].dtype=='float64']\n",
    "\n",
    "labels = []\n",
    "values = []\n",
    "for col in x_cols:\n",
    "    labels.append(col)\n",
    "    values.append(np.corrcoef(train_df_new[col].values, train_df_new.logerror.values)[0,1])\n",
    "corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n",
    "corr_df = corr_df.sort_values(by='corr_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [go.Bar(\n",
    "            x = np.array(corr_df.corr_values.values),\n",
    "            y = corr_df['col_labels'],\n",
    "            orientation = 'h',\n",
    "            )]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = \"Correlation coefficient of the variables\",\n",
    "              xaxis = dict(title = \"Correlation coefficient\"),\n",
    "              yaxis = dict(tickangle=35, \n",
    "                           tickfont=dict(size=9)),\n",
    "              font  = dict(size=12),\n",
    "              autosize = False,\n",
    "              width = 900,\n",
    "              height = 990,\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "92410577-02ba-d92d-b6b3-3673819ede1f",
    "_uuid": "00b0f4412075fc6442e55527d33278e066ade9ea"
   },
   "source": [
    "The correlation of the target variable with the given set of variables is low overall. \n",
    "\n",
    "A few variables at the top of this graph has no correlation values. There may be only one unique value and hence no correlation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2331b242-994c-8865-4fc8-13a62f518788",
    "_uuid": "b18517e88c3fb97b78a4eb9175cb86e3260d1fc4",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_zero_cols = ['assessmentyear', 'storytypeid', 'pooltypeid2', 'pooltypeid7', 'pooltypeid10', 'poolcnt', 'decktypeid', 'buildingclasstypeid']\n",
    "for col in corr_zero_cols:\n",
    "    print(col, len(train_df_new[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "927f3dfd-3461-a058-e70d-b23719b45b57",
    "_uuid": "965cbbc3b843d6778b482def1f628b1739660486"
   },
   "source": [
    "Check out variables with high correlation values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7971e8c3-69c3-7797-9ab4-04d60f2cc889",
    "_uuid": "2a7e44cce354285e88e5ade56ff1de7e22f333e2",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_df_sel = corr_df.ix[(corr_df['corr_values']>0.02) | (corr_df['corr_values'] < -0.01)]\n",
    "corr_df_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "78e253cf-ed6a-4e64-834d-f8c2c2ac1788",
    "_uuid": "fe9ea47714fa523f6d49ce476bba6da3307f3323",
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols_to_use = corr_df_sel.col_labels.tolist()\n",
    "\n",
    "temp_df = train_df[cols_to_use]\n",
    "corrmat = temp_df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trace = go.Heatmap(z=np.array(corrmat),\n",
    "                   x = cols_to_use,\n",
    "                   y = cols_to_use,\n",
    "                   #colorscale= 'Jet')\n",
    "                   colorscale=[[0.0000000000000000, 'rgb(165,0,38)'],    [0.1111111111111111, 'rgb(215,48,39)'],\n",
    "                              [0.2222222222222222, 'rgb(244,109,67)'],  [0.3333333333333333, 'rgb(253,174,97)'], \n",
    "                              [0.4444444444444444, 'rgb(254,224,144)'], [0.5555555555555556, 'rgb(224,243,248)'], \n",
    "                              [0.6666666666666666, 'rgb(171,217,233)'], [0.7777777777777778, 'rgb(116,173,209)'], \n",
    "                              [0.8888888888888888, 'rgb(69,117,180)'],  [1.0000000000000000, 'rgb(49,54,149)']],)\n",
    "                   #colorscale=[[1.0000000000000000, 'rgb(165,0,38)'],    [0.8888888888888888, 'rgb(215,48,39)'],\n",
    "                   #            [0.7777777777777778, 'rgb(244,109,67)'],  [0.6666666666666666, 'rgb(253,174,97)'], \n",
    "                   #            [0.5555555555555556, 'rgb(254,224,144)'], [0.4444444444444444, 'rgb(224,243,248)'], \n",
    "                   #            [0.3333333333333333, 'rgb(171,217,233)'], [0.2222222222222222, 'rgb(116,173,209)'], \n",
    "                   #            [0.1111111111111111, 'rgb(69,117,180)'],  [0.0000000000000000, 'rgb(49,54,149)']],)\n",
    "data=[trace]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = \"Important variables correlation map\",\n",
    "              font  = dict(size=12),\n",
    "              autosize = False,\n",
    "              width = 500,\n",
    "              height = 500,\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "87caaa5f-ed65-6ec1-75e8-af450304f966",
    "_uuid": "c6e3e1fff0d5b4a2a02dcd8215eb7143abb9fc53"
   },
   "source": [
    "Let us now look at each of them. Investigate individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "936de827-6302-efb1-993e-be2c113e28bd",
    "_uuid": "c5ee82420af622f1e4dc4d6eb784137e92441811"
   },
   "source": [
    "**Bathroom Count:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Histogram(\n",
    "        x=train_df['bathroomcnt'],\n",
    "        histnorm='count',\n",
    "        #marker=dict(colorscale='Jet',),\n",
    "        #opacity=0.75\n",
    "    )]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Frequency of Bathroom count',\n",
    "              xaxis = dict(title = 'Bathroom'),\n",
    "              yaxis = dict(title = 'Count'),\n",
    "              font  = dict(size=16),\n",
    "              autosize = False,\n",
    "              width = 800,\n",
    "              height = 500,\n",
    "              bargap=0.2,\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src = list(train_df['bathroomcnt'].values) \n",
    "result_dict = dict( [ (i, src.count(i)) for i in set(src) ] )\n",
    "f = train_df.sort_values(by=['bathroomcnt'], ascending=[True])\n",
    "N = len(result_dict)    # Number of boxes\n",
    "nbathroom = sorted(result_dict.keys())\n",
    "# generate an array of rainbow colors by fixing the saturation and lightness of the HSL representation of colour \n",
    "# and marching around the hue. \n",
    "# Plotly accepts any CSS color format, see e.g. http://www.w3schools.com/cssref/css_colors_legal.asp.\n",
    "c = ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 360, N)]\n",
    "\n",
    "# Each box is represented by a dict that contains the data, the type, and the colour. \n",
    "# Use list comprehension to describe N boxes, each with a different colour and with different randomly generated data:\n",
    "data = [{\n",
    "    'x': nbathroom[i],\n",
    "    'y': f['logerror'][f['bathroomcnt']==nbathroom[i]],\n",
    "    'name':nbathroom[i],\n",
    "    'type':'box',\n",
    "    'marker':{'color': c[i]}\n",
    "    } for i in range(int(N))]\n",
    "\n",
    "# format the layout\n",
    "#layout = {'xaxis': {'showgrid':False,'zeroline':False, 'tickangle':60,'showticklabels':False},\n",
    "#          'yaxis': {'zeroline':False,'gridcolor':'white'},\n",
    "#          'paper_bgcolor': 'rgb(233,233,233)',\n",
    "#          'plot_bgcolor': 'rgb(233,233,233)',\n",
    "#          }\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = \"Logerror vs Bathroom count\",\n",
    "              xaxis = dict(title = \"Bathroom count\"),\n",
    "              yaxis = dict(title = \"Logerror\",\n",
    "                           tickangle=0, \n",
    "                           tickfont=dict(size=9)),\n",
    "              font  = dict(size=12),\n",
    "              autosize = False,\n",
    "              width = 900,\n",
    "              height = 700,\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    go.Histogram(\n",
    "        x=train_df['bedroomcnt'],\n",
    "        histnorm='count',\n",
    "        #marker=dict(colorscale='Jet',),\n",
    "        #opacity=0.75\n",
    "    )]\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title = 'Frequency of Bedroom count',\n",
    "              xaxis = dict(title = 'Bedroom count'),\n",
    "              yaxis = dict(title = 'Frequency'),\n",
    "              font  = dict(size=16),\n",
    "              autosize = False,\n",
    "              width = 800,\n",
    "              height = 500,\n",
    "              bargap=0.2,\n",
    "              )\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.03 is the mean value with which we replaced the Null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['bedroomcnt'].ix[train_df['bedroomcnt']>7] = 7\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.violinplot(x='bedroomcnt', y='logerror', data=train_df)\n",
    "plt.xlabel('Bedroom count', fontsize=12)\n",
    "plt.ylabel('Log Error', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['bedroomcnt'].ix[train_df['bedroomcnt']>7] = 7\n",
    "fig = ff.create_violin(train_df, data_header='logerror', group_header='bedroomcnt')\n",
    "\n",
    "# Edit the layout\n",
    "layout = dict(title='Log Error vs Bedroom count',\n",
    "              xaxis = dict(title = 'Bedroom count'),\n",
    "              yaxis = dict(title = 'Log Error'),\n",
    "              font  = dict(size=16),\n",
    "              autosize = False,\n",
    "              width = 800,\n",
    "              height = 500,\n",
    "              bargap=0.2,\n",
    "              )\n",
    "iplot(fig, layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col = \"taxamount\"\n",
    "ulimit = np.percentile(train_df[col].values, 99)\n",
    "llimit = np.percentile(train_df[col].values, 1)\n",
    "train_df[col].ix[train_df[col]>ulimit] = ulimit\n",
    "train_df[col].ix[train_df[col]<llimit] = llimit\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.jointplot(x=train_df['taxamount'].values, y=train_df['logerror'].values, size=10, color='g')\n",
    "plt.ylabel('Log Error', fontsize=12)\n",
    "plt.xlabel('Tax Amount', fontsize=12)\n",
    "plt.title(\"Tax Amount Vs Log error\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "ggplot(aes(x='yearbuilt', y='logerror'), data=train_df) + \\\n",
    "    geom_point(color='steelblue', size=1) + \\\n",
    "    stat_smooth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x='latitude', y='longitude', color='logerror'), data=train_df) + \\\n",
    "    geom_point() + \\\n",
    "    scale_color_gradient(low = 'red', high = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ggplot(aes(x='finishedsquarefeet12', y='taxamount', color='logerror'), data=train_df) + \\\n",
    "    geom_point(alpha=0.7) + \\\n",
    "    scale_color_gradient(low = 'pink', high = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "train_y = train_df['logerror'].values\n",
    "cat_cols = [\"hashottuborspa\", \"propertycountylandusecode\", \"propertyzoningdesc\", \"fireplaceflag\", \"taxdelinquencyflag\"]\n",
    "train_df = train_df.drop(['parcelid', 'logerror', 'transactiondate', 'transaction_month']+cat_cols, axis=1)\n",
    "feat_names = train_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30, max_features=0.3, n_jobs=-1, random_state=0)\n",
    "model.fit(train_df, train_y)\n",
    "\n",
    "## plot the importances ##\n",
    "importances = model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1][:20]\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n",
    "plt.xlim([-1, len(indices)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(train_df, train_y,\n",
    "                                                test_size=0.3,\n",
    "                                                random_state=2017)\n",
    "\n",
    "\n",
    "\n",
    "model = ensemble.ExtraTreesRegressor(n_estimators=100, max_depth=30, max_features=0.3, n_jobs=-1, random_state=777)\n",
    "model.fit(xtrain, ytrain)\n",
    "pred = model.predict(xtest)\n",
    "mean_absolute_error(ytest, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print('Loading data ...')\n",
    "train  = pd.read_csv('train_2016_v2.csv')\n",
    "prop   = pd.read_csv('properties_2016.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "\n",
    "print('Binding to float32')\n",
    "for c, dtype in zip(prop.columns, prop.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        prop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "        \n",
    "print('Creating training set ...')\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "x_train = df_train.drop(['parcelid',\n",
    "                         'logerror',\n",
    "                         'transactiondate',\n",
    "                         'propertyzoningdesc',\n",
    "                         'propertycountylandusecode'], axis=1)\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "c1 = df_train['logerror'].mean()\n",
    "c2 = df_train['logerror'].median()\n",
    "print('Logerror mean: ' + str(c1))\n",
    "print('Logerror median: ' + str(c2))\n",
    "print('deleting df_train ...')\n",
    "del df_train; gc.collect()\n",
    "\n",
    "split = 80000\n",
    "x_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n",
    "\n",
    "mean_values  = x_train.mean(axis=0)\n",
    "x_train = x_train.fillna(mean_values, inplace=True)\n",
    "\n",
    "#model = ensemble.ExtraTreesRegressor(n_estimators=100, max_depth=30, max_features=0.3, n_jobs=-1, random_state=777)\n",
    "\n",
    "parameters = {'n_estimators':[50, 100], 'max_depth':[10, 30], 'max_features':[0.3]}\n",
    "extratree = ensemble.ExtraTreesRegressor() #criterion='mae'\n",
    "model = GridSearchCV(extratree, parameters,verbose=2,scoring=make_scorer(mean_absolute_error))\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.cv_results_\n",
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_values  = x_valid.mean(axis=0)\n",
    "x_valid = x_valid.fillna(mean_values, inplace=True)\n",
    "\n",
    "dummy1 = DummyClassifier(constant=c1,random_state=0)\n",
    "dummy2 = DummyClassifier(constant=c2,random_state=0)\n",
    "\n",
    "dummy1.fit(x_train, y_train)\n",
    "dummy2.fit(x_train, y_train)\n",
    "\n",
    "print('Predicting on validation set ...')\n",
    "pred  = model.predict(x_valid)\n",
    "pred1 = dummy1.predict(x_valid)\n",
    "pred2 = dummy2.predict(x_valid)\n",
    "print('Model validation set MAE: '                 + str(mean_absolute_error(y_valid, pred)))\n",
    "print('Mean Dummy regressor validation set MAE: '  + str(mean_absolute_error(y_valid, pred1)))\n",
    "print('Median Dummy regressor validation set MAE: '+ str(mean_absolute_error(y_valid, pred2)))\n",
    "\n",
    "\n",
    "print('Building test set ...')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "\n",
    "\n",
    "print('x_test ...')\n",
    "x_test = df_test[train_columns]\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "    \n",
    "    \n",
    "print('Fill NA ...')\n",
    "mean_values  = x_test.mean(axis=0)\n",
    "x_test = x_test.fillna(mean_values, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print('Predicting on test ...')\n",
    "p_test = model.predict(x_test)\n",
    "#p_test = 0.97*p_test + 0.03*0.011\n",
    "\n",
    "print('deleting x_test ...')\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = p_test\n",
    "\n",
    "end = time.time()\n",
    "print('time taken: ' + str(end - start))\n",
    "\n",
    "print('Writing csv ...')\n",
    "sub.to_csv('ExtraTrees_starter.csv', index=False, float_format='%.4f') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "print(xgb.__version__)\n",
    "xgb_params = {\n",
    "    'eta': 0.05,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.7,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'objective': 'reg:linear',\n",
    "    'silent': 1,\n",
    "    'seed' : 0\n",
    "}\n",
    "dtrain = xgb.DMatrix(train_df, train_y, feature_names=train_df.columns.values)\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n",
    "\n",
    "# plot the important features #\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "\n",
    "print('Loading data ...')\n",
    "train  = pd.read_csv('train_2016_v2.csv')\n",
    "prop   = pd.read_csv('properties_2016.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "print('Binding to float32')\n",
    "\n",
    "for c, dtype in zip(prop.columns, prop.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        prop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "print('Creating training set ...')\n",
    "\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "x_train = df_train.drop(['parcelid',\n",
    "                         'logerror',\n",
    "                         'transactiondate',\n",
    "                         'propertyzoningdesc',\n",
    "                         'propertycountylandusecode'], axis=1)\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train; gc.collect()\n",
    "\n",
    "split = 80000\n",
    "x_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n",
    "\n",
    "print('Building DMatrix...')\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "\n",
    "del x_train; gc.collect() #, x_valid\n",
    "\n",
    "print('Training ...')\n",
    "\n",
    "params = {\n",
    "    'eta'         : 0.02,\n",
    "    'objective'   : 'reg:linear',\n",
    "    'eval_metric' : 'mae',\n",
    "    'max_depth'   : 4,\n",
    "    'silent'      : 1,\n",
    "}\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "clf = xgb.train(params,\n",
    "                d_train,\n",
    "                10000,\n",
    "                watchlist,\n",
    "                early_stopping_rounds=100,\n",
    "                verbose_eval=10)\n",
    "\n",
    "del d_train #, d_valid\n",
    "\n",
    "print('Building test set ...')\n",
    "\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "\n",
    "del prop; gc.collect()\n",
    "\n",
    "x_test = df_test[train_columns]\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "\n",
    "del df_test, sample; gc.collect()\n",
    "\n",
    "d_test = xgb.DMatrix(x_test)\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "\n",
    "print('Predicting on validation set ...')\n",
    "pred  = clf.predict(d_valid)\n",
    "pred1 = dummy1.predict(x_valid)\n",
    "pred2 = dummy2.predict(x_valid)\n",
    "print('Validation set MAE:                       '+ str(mean_absolute_error(y_valid, pred)))\n",
    "print('Mean Dummy regressor validation set MAE: ' + str(mean_absolute_error(y_valid, pred1)))\n",
    "print('Median Dummy regresor validation set MAE: '+ str(mean_absolute_error(y_valid, pred2)))\n",
    "\n",
    "\n",
    "print('Predicting on test ...')\n",
    "\n",
    "p_test = clf.predict(d_test)\n",
    "p_test = 0.97*p_test + 0.03*0.011\n",
    "\n",
    "del d_test; gc.collect()\n",
    "\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = p_test\n",
    "\n",
    "print('Writing csv ...')\n",
    "sub.to_csv('xgb_starter.csv', index=False, float_format='%.4f') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Any results you write to the current directory are saved as output.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "\n",
    "print('Loading data ...')\n",
    "train = pd.read_csv('train_2016_v2.csv')\n",
    "prop  = pd.read_csv('properties_2016.csv')\n",
    "\n",
    "for c, dtype in zip(prop.columns, prop.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        prop[c] = prop[c].astype(np.float32)\n",
    "\n",
    "df_train = train.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "x_train = df_train.drop(['parcelid',\n",
    "                         'logerror',\n",
    "                         'transactiondate',\n",
    "                         'propertyzoningdesc',\n",
    "                         'propertycountylandusecode'], axis=1)\n",
    "\n",
    "y_train = df_train['logerror'].values\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "train_columns = x_train.columns\n",
    "\n",
    "for c in x_train.dtypes[x_train.dtypes == object].index.values:\n",
    "    x_train[c] = (x_train[c] == True)\n",
    "\n",
    "del df_train; gc.collect()\n",
    "\n",
    "split = 80000\n",
    "x_train, y_train, x_valid, y_valid = x_train[:split], y_train[:split], x_train[split:], y_train[split:]\n",
    "x_train = x_train.values.astype(np.float32, copy=False)\n",
    "x_valid = x_valid.values.astype(np.float32, copy=False)\n",
    "\n",
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)\n",
    "\n",
    "params = {\n",
    "    'max_bin'         : 20,\n",
    "    'learning_rate'   : 0.0021,        # shrinkage_rate\n",
    "    'boosting_type'   : 'gbdt',\n",
    "    'objective'       : 'regression',\n",
    "    'metric'          : 'l1',          # or 'mae'\n",
    "    'sub_feature'     : 0.5,           # feature_fraction\n",
    "    'bagging_fraction': 0.85,          # sub_row\n",
    "    'bagging_freq'    : 40,\n",
    "    'num_leaves'      : 512,           # num_leaf\n",
    "    'min_data'        : 500,           # min_data_in_leaf\n",
    "    'min_hessian'     : 0.05,          # min_sum_hessian_in_leaf\n",
    "          }\n",
    "\n",
    "\n",
    "watchlist = [d_valid]\n",
    "clf       = lgb.train(params, d_train, 500, watchlist)\n",
    "\n",
    "del d_train, d_valid; gc.collect()\n",
    "del x_train; gc.collect()  #, x_valid\n",
    "\n",
    "print(\"Prepare for the prediction ...\")\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "sample['parcelid'] = sample['ParcelId']\n",
    "\n",
    "df_test = sample.merge(prop, on='parcelid', how='left')\n",
    "del sample, prop; gc.collect()\n",
    "\n",
    "x_test = df_test[train_columns]\n",
    "del df_test; gc.collect()\n",
    "\n",
    "for c in x_test.dtypes[x_test.dtypes == object].index.values:\n",
    "    x_test[c] = (x_test[c] == True)\n",
    "x_test = x_test.values.astype(np.float32, copy=False)\n",
    "\n",
    "print('Predicting on validation set ...')\n",
    "pred = clf.predict(x_valid)\n",
    "print('Validation set MAE: '+ str(mean_absolute_error(y_valid, pred)))\n",
    "\n",
    "print(\"Start prediction ...\")\n",
    "# num_threads > 1 will predict very slow in kernel\n",
    "clf.reset_parameter({\"num_threads\":1})\n",
    "p_test = clf.predict(x_test)\n",
    "p_test = 0.97*p_test + 0.03*0.011\n",
    "\n",
    "del x_test; gc.collect()\n",
    "\n",
    "print(\"Start write result ...\")\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = p_test\n",
    "\n",
    "sub.to_csv('lgb_starter_1.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
